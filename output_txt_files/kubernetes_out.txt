<paste log output from Kubernetes-mediated prediction, here>
## First time run kubernetes
Sang Pham@SangPham MINGW64 ~/workspace/project-ml-microservice-kubernetes (dev)
$ ./run_kubernetes.sh
pod/app created
NAME   READY   STATUS              RESTARTS   AGE
app    0/1     ContainerCreating   0          3s
error: unable to forward port because pod is not running. Current status=Pending

## Get pods
Sang Pham@SangPham MINGW64 ~/workspace/project-ml-microservice-kubernetes (dev)
$ kubectl get pod
NAME   READY   STATUS    RESTARTS   AGE
app    1/1     Running   0          2m32s

## Second time run kubernetes
Sang Pham@SangPham MINGW64 ~/workspace/project-ml-microservice-kubernetes (dev)
$ ./run_kubernetes.sh
Error from server (AlreadyExists): pods "app" already exists
NAME   READY   STATUS    RESTARTS   AGE
app    1/1     Running   0          2m45s
Forwarding from 127.0.0.1:8080 -> 80

## Make prediction log
Sang Pham@SangPham MINGW64 ~/workspace/project-ml-microservice-kubernetes (dev)
$ kubectl logs pods/app
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 301-720-004
 [2023-09-18 18:35:28,831] INFO in app: JSON payload:
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2023-09-18 18:35:28,849] INFO in app: Inference payload DataFrame:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-09-18 18:35:28,862] INFO in app: Scaling Payload:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-09-18 18:35:28,871] INFO in app: output prediction:
[20.35373177134412]
172.17.0.1 - - [18/Sep/2023 18:35:28] "POST /predict HTTP/1.1" 200 -

## Log when run make_predictions.sh
Sang Pham@SangPham MINGW64 ~/workspace/project-ml-microservice-kubernetes (dev)
$ ./make_prediction.sh
Port: 8080
{
  "prediction": [
    20.35373177134412
  ]
}